Kubeadm是Kubernetes主推的部署工具之一。用户只要通过两条指令就可以完成一个Kubernetes集群的部署

~~~bash
# 创建一个 Master 节点
# kubeadm init

# 将一个 Node 节点加入到当前集群中
# kubeadm join <Master 节点的 IP 和端口 >
~~~

# kubeadm工作原理

### 安装Master

初始化 master 时，只需要执行 kubeadm init 指令即可，但强烈推荐在使用该命令部署master节点时，使用如下指令

~~~bash
# kubeadm init --config kubeadm.yaml
~~~

该命令会自动执行如下操作

- 系统状态检查
- 生成 token
- 生成自签名 CA 和 client 端证书
- 生成 kubeconfig 用于 kubelet 连接 API server
- 为 Master 组件生成 Static Pod manifests，并放到 `/etc/kubernetes/manifests` 目录中
- 配置 RBAC 并设置 Master node 只运行控制平面组件
- 创建附加服务，比如 kube-proxy 和 kube-dns

kubeadm init工作机制

![init工作机制](./images/kubeadm_init_flow_diagram.jfif)

### 配置Node

加入集群节点，需要执行kubeadm join指令

~~~bash
kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>
~~~

该命令会自动执行如下操作

- 从 API server 下载 CA
- 创建本地证书，并请求 API Server 签名
- 最后配置 kubelet 连接到 API Server

在node节点加入集群时，需要建立双向信任，它分成两部分：discovery（node信任master）和TLS bootstrap（master信任node）。

* discovery
  node连接master时使用discovery token和CA public key hash，用来获取集群CA证书并完成对master的信任过程，确认所连接的master是所期望的合法master。
* TLS bootstrap
  node连接master时携带TLS bootstrap token（通常可以和discovery token使用同一个），用于kubelet向master发起certificate signing request（CSR）时的临时认证，kubeadm默认设置了master自动批准该CSR。当该CSR得到批准后，master便会为kubelet签发客户端证书，用于后续kubelet访问apiserver时的身份认证。

kubeadm join工作机制

![kubeadm join工作机制](./images/kubeadm_join_flow_diagram.jfif)

# kubeadm安装k8s集群实例

### 实例说明

软件

```
vagrant
```

服务器

```
master,etc:192.167.61.11
node:192.168.61.12
node:192.168.61.13
```

### 环境准备

通过vagrant构建服务器环境

```vagrant
# -*- mode: ruby -*-
# vi: set ft=ruby :

$num_instances = 3

Vagrant.configure("2") do |config|
  config.vm.box_check_update = false
  config.vm.synced_folder ".", "/vagrant", type: "rsync"
    (1..$num_instances).each do |id|
    config.vm.define "node#{id}" do |node|
      node.vm.box="centos/7"
      node.ssh.insert_key = false
      node.vm.hostname = "node#{id}"
      node.vm.network :private_network, ip: "192.168.61.#{id+10}"
      node.vm.provider :virtualbox do |vb, override|
        vb.name = "node#{id}"
        vb.gui = false
        vb.memory = 4096
        vb.cpus = 2
      end
    end
  end
end
```

系统环境配置

```bash
# 关闭selinux,修改/etc/sysconfig/selinux文件设置
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
setenforce 0

# 关闭swap,注释/etc/fstab文件里swap相关的行。Kubernetes1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动
swapoff -a
sed -i 's/.*swap.*/#&/' /etc/fstab

# 开启forward
iptables -P FORWARD ACCEPT

# 关闭防火墙
systemctl stop firewalld
systemctl disable firewalld

# 配置转发相关参数
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness = 0
EOF
sysctl --system

# 加载ipvs相关内核模块,如果重新开机,需要重新加载
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh
modprobe nf_conntrack_ipv4
modprobe br_netfilter
lsmod | grep ip_vs

# 配置开启自加载
cat >>/etc/modules-load.d/k8s-ipvs.conf<<EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
br_netfilter
EOF

# 配置hosts解析
cat >>/etc/hosts<<EOF
192.168.61.11   node1
192.168.61.12   node2
192.168.61.13   node3
EOF

# 配置kubernetes为阿里yum源
cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF
```

安装Kubeadm和Docker

```bash
# 配置docker使用阿里yum源
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
yum clean all && yum makecache

# 安装docker安装依赖
yum install -y yum-utils device-mapper-persistent-data lvm2

# 安装docker、kubeadm、kubelet、kubectl
yum install -y docker-ce kubelet kubeadm kubectl kubernetes-cni --disableexcludes=kubernetes ipvsadm

# 开启docker
systemctl enable docker && systemctl start docker
```

配置启动kubelet

~~~bash
# 获取docker的cgroups
DOCKER_CGROUPS=$(docker info | grep 'Cgroup' | cut -d' ' -f3)
echo $DOCKER_CGROUPS
# 配置kubelet使用国内pause镜像
# 配置kubelet的cgroups
cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS="--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"
EOF

# 启动
systemctl daemon-reload
systemctl enable kubelet && systemctl start kubelet
~~~

### 使用kubeadm init配置master节点

编写部署配置YAML文件（kubeadm.yaml）

~~~yaml
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.12.1
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
api:
  advertiseAddress: 192.168.61.11

controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: 10s
apiServerExtraArgs:
  runtime-config: "api/all=true"
~~~

执行如下指令进行Kubernetes master部署，如果不能正常连接到Google镜像可以参考[基于国内环境无法访问Google镜像](#基于国内环境无法访问Google镜像)先将kubeadm需要的镜像下载到本地再进行部署安装

~~~bash
# kubeadm init --config kubeadm.yaml
[init] using Kubernetes version: v1.12.1
[preflight] running pre-flight checks
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[preflight] Activating the kubelet service
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [node1 localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [node1 localhost] and IPs [192.168.61.11 127.0.0.1 ::1]
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [node1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.61.11]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[certificates] Generated sa key and public key.
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests"
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 44.010340 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.12" in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node node1 as master by adding the label "node-role.kubernetes.io/master=''"
[markmaster] Marking the node node1 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node1" as an annotation
[bootstraptoken] using token: smcy44.3vwgw8pa18qu1bjx
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.61.11:6443 --token smcy44.3vwgw8pa18qu1bjx --discovery-token-ca-cert-hash sha256:01cabac5538359642543b22f39feebc893785e4d5bc3c08a200f5a49af8cba53                               
~~~

> 根据安装输出可以看出kabeadm安装Kubernetes的一些关键步骤
>
> - [kubelet] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”
>
> - [certificates]生成相关的各种证书
>
> - [kubeconfig]生成admin，kubelet，controller-manager，scheduler相关的kubeconfig文件
>
> - [bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
>
> - 下面的命令是配置常规用户如何使用kubectl访问集群：
>
>   ```
>   mkdir -p $HOME/.kube
>   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
>   sudo chown $(id -u):$(id -g) $HOME/.kube/config
>   ```
>
> - 将节点加入集群的命令
>
>   kubeadm join 192.168.61.11:6443 --token smcy44.3vwgw8pa18qu1bjx --discovery-token-ca-cert-hash sha256:01cabac5538359642543b22f39feebc893785e4d5bc3c08a200f5a49af8cba53 

第一次使用Kubernetes集群需要使用配置如下指令。因为，Kubernetes集群默认需要加密方式访问，所以这几条指令就是将刚刚部署生产的Kubernetes集群的安全配置文件保存到当前用户的.kube目录下，kubectl默认会使用这个目录下的授权信息访问Kubernetes集群

~~~bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
~~~

使用kubeclt get指令查看集群状态

~~~bash
# kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
# kubectl get nodes
NAME    STATUS     ROLES    AGE   VERSION
node1   NotReady   master   31m   v1.12.1
~~~

集群初始化如果遇到问题，可以使用下面的指令进行清理

~~~bash
# kubeadm reset
~~~

查看集群master节点状态状态是NotReady。通过调试Kubernetes集群最重要的手段就是用kubectl describe查看节点（Node）对象的详细信息、状态和事件（Event）

~~~bash
# kubectl describe node node1
...
Conditions:                                                                             ...                          
  Ready            False   Sat, 20 Oct 2018 03:25:25 +0000   Sat, 20 Oct 2018 03:00:41 +0000   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized  
~~~

通过kubectl describe指令的输出可以看到master节点NotReady的原因在于没有部署任何的网络插件。再通过kubectl查看节点上各个系统Pod的状态，coredns依赖网络的Pod都处于ContainerCreating状态。

~~~bash
# kubectl get pods -n kube-system
NAME                            READY   STATUS              RESTARTS   AGE
coredns-6c66ffc55b-8mds8        0/1     ContainerCreating   0          47m
coredns-6c66ffc55b-dc8tp        0/1     ContainerCreating   0          47m
etcd-node1                      1/1     Running             0          46m
kube-apiserver-node1            1/1     Running             0          46m
kube-controller-manager-node1   1/1     Running             0          46m
kube-proxy-w62b8                1/1     Running             0          47m
kube-scheduler-node1            1/1     Running             0          46m
~~~

部署网络插件

~~~bash
# export kubever=$(kubectl version | base64 | tr -d '\n')
# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"
~~~

通过kubectl get指令重新查看集群的信息

~~~bash
# kubectl get pods -n kube-system -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE
coredns-6c66ffc55b-8mds8        1/1     Running   0          59m     10.32.0.3       node1   <none>
coredns-6c66ffc55b-dc8tp        1/1     Running   0          59m     10.32.0.2       node1   <none>
etcd-node1                      1/1     Running   0          59m     192.168.61.11   node1   <none>
kube-apiserver-node1            1/1     Running   0          58m     192.168.61.11   node1   <none>
kube-controller-manager-node1   1/1     Running   0          59m     192.168.61.11   node1   <none>
kube-proxy-w62b8                1/1     Running   0          59m     192.168.61.11   node1   <none>
kube-scheduler-node1            1/1     Running   0          58m     192.168.61.11   node1   <none>
weave-net-x4s5f                 2/2     Running   0          5m11s   192.168.61.11   node1   <none>
# kubectl get nodes
NAME    STATUS   ROLES    AGE   VERSION
node1   Ready    master   55m   v1.12.1
~~~

### 使用kubeadm join添加worker节点

Kubernetes的worker节点跟Master节点几乎是相同，它们运行着的都是一个kubelet组件。唯一的区别在于kubectl init过程中，kubelet启动后，Master节点还会自动运行kube-apiserver、kube-scheduler、kube-controller-manager三个系统Pod。

加入worker节点需要用到kube-proxy，pause两个组件，需要确保服务器能正常拉取到这两个组件

在node2，node3节点上安装网络插件

~~~bash
# export kubever=$(kubectl version | base64 | tr -d '\n')
# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"
~~~

在node2，node3节点上执行部署master节点时生成的kubeadm join指令

~~~bash
kubeadm join 192.168.61.11:6443 --token smcy44.3vwgw8pa18qu1bjx --discovery-token-ca-cert-hash sha256:01cabac5538359642543b22f39feebc893785e4d5bc3c08a200f5a49af8cba53
~~~

通过kubectl指令查看整个集群信息

~~~bash
# kubectl get pods -n kube-system -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE
coredns-6c66ffc55b-8mds8        1/1     Running   0          127m    10.32.0.3       node1   <none>
coredns-6c66ffc55b-dc8tp        1/1     Running   0          127m    10.32.0.2       node1   <none>
etcd-node1                      1/1     Running   0          127m    192.168.61.11   node1   <none>
kube-apiserver-node1            1/1     Running   0          127m    192.168.61.11   node1   <none>
kube-controller-manager-node1   1/1     Running   0          127m    192.168.61.11   node1   <none>
kube-proxy-g8rh6                1/1     Running   0          2m28s   192.168.61.13   node3   <none>
kube-proxy-w62b8                1/1     Running   0          127m    192.168.61.11   node1   <none>
kube-proxy-x857g                1/1     Running   0          3m25s   192.168.61.12   node2   <none>
kube-scheduler-node1            1/1     Running   0          127m    192.168.61.11   node1   <none>
weave-net-47xjh                 2/2     Running   3          2m28s   192.168.61.13   node3   <none>
weave-net-p2vs9                 2/2     Running   4          3m25s   192.168.61.12   node2   <none>
weave-net-x4s5f                 2/2     Running   0          73m     192.168.61.11   node1   <none>
# kubectl get nodes
NAME    STATUS     ROLES    AGE     VERSION
node1   Ready      master   128m    v1.12.1
node2   Ready      <none>   3m33s   v1.12.1
node3   Ready      <none>   2m36s   v1.12.1
~~~

### Master node参与工作负载

默认情况下Master节点是不允许运行用户Pod，出于安全考虑Pod不会被调度到Master Node上，也就是说Master Node不参与工作负载。Kubernetes做到这一点，依靠的是Kubernetes的Taint/Toleration机制。

原理非常简单：一旦某个节点被加上了一个Taint，即被“打上了污点”，那么所有Pod就都不能在这个节点上运行，因为Kubernetes的Pod都有“洁癖”。除非有个别的Pod申明明自己能“容忍”这个“污点”，即申明了Toleration，它才可以在这个节点上运行。

~~~bash
# kubectl describe node node1 | grep Taint
Taints:             node-role.kubernetes.io/master:NoSchedule
~~~

这里搭建的是测试环境，去掉这个污点使Master参与工作负载。在这个键后面加上一个短横线“-”，表示移除以“node-role.kubernetes.io/master”为键的Taint。

```bash
# kubectl taint nodes node1 node-role.kubernetes.io/master-
node "node1" untainted
```

### 测试器间的通信DNS

~~~bash
# kubectl run curl --image=radial/busyboxplus:curl -it
kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.
If you don't see a command prompt, try pressing enter.
[ root@curl-5cc7b478b6-b2hmp:/ ]$ nslookup kubernetes.default
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.96.0.1 kubernetes.default.svc.cluster.local
~~~

### 从集群中移除Worker

在master节点执行如下指令

~~~bash
# kubectl drain node2 --delete-local-data --force --ignore-daemonsets
# kubectl delete node node2
~~~

在node2节点上执行如下指令

~~~bash
# kubeadm reset
~~~

### 常用命令

获取Token（Dashboard插件通过Token登录）

~~~bash
# kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
~~~

调试Kubernetes集群

~~~bash
# 查看具体问题
# kubectl describe pod <kubernetes-pod> -n kube-system

# 查看日志
# kubectl logs -n kube-system <weave-net-pod> weave

# 查看服务启动情况
# journalctl -xef -u kubelet -n 20

# 网络问题查看
# ip route
# ip -4 -o addr
# iptables-save
# iptables -nvL

# 忘记初始master节点时的node节点加入集群命令
# 简单方法
# kubeadm token create --print-join-command
# 第二种方法
# token=$(kubeadm token generate)
# kubeadm token create $token --print-join-command --ttl=0
~~~

### 基于国内环境无法访问Google镜像

* Google镜像：https://console.cloud.google.com/gcr/images/google-containers?project=google-containers (需要翻墙)

- 阿里云镜像：https://dev.aliyun.com/list.html?namePrefix=google-containers
- Github个人：https://github.com/anjia0532/gcr.io_mirror

获取当前版本kubeadm 启动需要的镜像

~~~bash
# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.12.1
k8s.gcr.io/kube-controller-manager:v1.12.1
k8s.gcr.io/kube-scheduler:v1.12.1
k8s.gcr.io/kube-proxy:v1.12.1
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.2
~~~

制作镜像拉取脚本

~~~bash
# vi pullimages.sh
#!/bin/bash
images=(
    kube-apiserver-amd64:v1.12.1
    kube-controller-manager-amd64:v1.12.1
    kube-scheduler-amd64:v1.12.1
    kube-proxy-amd64:v1.12.1
    pause-amd64:3.1
    etcd-amd64:3.2.24
    coredns:1.2.2
)

for imageName in ${images[@]} ; do
    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
~~~

